{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf680099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a1a0ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 200\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9723d783",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, neg_penalty):\n",
    "        super(GCN, self).__init__()\n",
    "        self.in_dim = in_dim  # 输入的维度\n",
    "        self.out_dim = out_dim  # 输出的维度\n",
    "        self.neg_penalty = neg_penalty  # 负值\n",
    "        self.kernel = nn.Parameter(torch.FloatTensor(in_dim, out_dim))\n",
    "        self.c = 0.85\n",
    "        self.losses = []\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        # GCN-node\n",
    "        feature_dim = int(adj.shape[-1])\n",
    "        eye = torch.eye(feature_dim).to(device)  # 生成对角矩阵 feature_dim * feature_dim\n",
    "        if x is None:  # 如果没有初始特征\n",
    "            AXW = torch.tensordot(\n",
    "                adj, self.kernel, [[-1], [0]]\n",
    "            )  # batch_size * num_node * feature_dim\n",
    "        else:\n",
    "            XW = torch.tensordot(\n",
    "                x, self.kernel, [[-1], [0]]\n",
    "            )  # batch *  num_node * feature_dim\n",
    "            AXW = torch.matmul(adj, XW)  # batch *  num_node * feature_dim\n",
    "        # I_cAXW = eye+self.c*AXW\n",
    "        I_cAXW = eye + self.c * AXW\n",
    "        y_relu = torch.nn.functional.relu(I_cAXW)\n",
    "        temp = torch.mean(input=y_relu, dim=-2, keepdim=True) + 1e-6\n",
    "        col_mean = temp.repeat([1, feature_dim, 1])\n",
    "        y_norm = torch.divide(y_relu, col_mean)  # 正则化后的值\n",
    "        output = torch.nn.functional.softplus(y_norm)\n",
    "        # output = y_relu\n",
    "        # 做个尝试\n",
    "        if self.neg_penalty != 0:\n",
    "            neg_loss = torch.multiply(\n",
    "                torch.tensor(self.neg_penalty),\n",
    "                torch.sum(torch.nn.functional.relu(1e-6 - self.kernel)),\n",
    "            )\n",
    "            self.losses.append(neg_loss)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eca8a22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_gnn(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(model_gnn, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        ###################################\n",
    "        self.gcn1_p = GCN(in_dim, hidden_dim, 0.2)\n",
    "        self.gcn2_p = GCN(hidden_dim, hidden_dim, 0.2)\n",
    "        # self.gcn3_p = GCN(in_dim, hidden_dim, 0.2)\n",
    "        self.gcn1_n = GCN(in_dim, hidden_dim, 0.2)\n",
    "        self.gcn2_n = GCN(hidden_dim, hidden_dim, 0.2)\n",
    "        # self.gcn3_n = GCN(in_dim, hidden_dim, 0.2)\n",
    "        # ----------------------------------\n",
    "        self.gcn1_p_1 = GCN(in_dim, hidden_dim, 0.2)\n",
    "        self.gcn2_p_1 = GCN(hidden_dim, hidden_dim, 0.2)\n",
    "        # self.gcn3_p_1 = GCN(in_dim, hidden_dim, 0.2)\n",
    "        self.gcn1_n_1 = GCN(in_dim, hidden_dim, 0.2)\n",
    "        self.gcn2_n_1 = GCN(hidden_dim, hidden_dim, 0.2)\n",
    "        # self.gcn3_n_1 = GCN(in_dim, hidden_dim, 0.2)\n",
    "        # ----------------------------------\n",
    "        self.gcn1_p_2 = GCN(in_dim, hidden_dim, 0.2)\n",
    "        self.gcn2_p_2 = GCN(hidden_dim, hidden_dim, 0.2)\n",
    "        # self.gcn3_p_2 = GCN(in_dim, hidden_dim, 0.2)\n",
    "        self.gcn1_n_2 = GCN(in_dim, hidden_dim, 0.2)\n",
    "        self.gcn2_n_2 = GCN(hidden_dim, hidden_dim, 0.2)\n",
    "        # self.gcn3_n_2 = GCN(in_dim, hidden_dim, 0.2)\n",
    "        # ---------------------------------\n",
    "        self.gcn_p_shared = GCN(in_dim, hidden_dim, 0.2)\n",
    "        self.gcn_n_shared = GCN(in_dim, hidden_dim, 0.2)\n",
    "        self.gcn_p_shared1 = GCN(in_dim, hidden_dim, 0.2)\n",
    "        self.gcn_n_shared1 = GCN(in_dim, hidden_dim, 0.2)\n",
    "        # --------------------------------- ATT score\n",
    "        self.Wp_1 = nn.Linear(self.hidden_dim, 1)\n",
    "        self.Wp_2 = nn.Linear(self.hidden_dim, 1)\n",
    "        self.Wp_3 = nn.Linear(self.hidden_dim, 1)\n",
    "        self.Wn_1 = nn.Linear(self.hidden_dim, 1)\n",
    "        self.Wn_2 = nn.Linear(self.hidden_dim, 1)\n",
    "        self.Wn_3 = nn.Linear(self.hidden_dim, 1)\n",
    "        ###################################\n",
    "        self.kernel_p = nn.Parameter(torch.FloatTensor(dim, in_dim))  #\n",
    "        self.kernel_n = nn.Parameter(torch.FloatTensor(dim, in_dim))\n",
    "        self.kernel_p1 = nn.Parameter(torch.FloatTensor(dim, in_dim))  #\n",
    "        self.kernel_n1 = nn.Parameter(torch.FloatTensor(dim, in_dim))\n",
    "        self.kernel_p2 = nn.Parameter(torch.FloatTensor(dim, in_dim))  #\n",
    "        self.kernel_n2 = nn.Parameter(torch.FloatTensor(dim, in_dim))\n",
    "        # print(self.kernel_p)\n",
    "        # self.kernel_p = Variable(torch.randn(116, 5)).cuda()  # 116 5\n",
    "        # self.kernel_n = Variable(torch.randn(116, 5)).cuda()   # 116 5\n",
    "        ################################################\n",
    "        self.lin1 = nn.Linear(2 * in_dim * in_dim, 16)\n",
    "        self.lin2 = nn.Linear(16, self.out_dim)\n",
    "        self.lin1_1 = nn.Linear(2 * in_dim * in_dim, 16)\n",
    "        self.lin2_1 = nn.Linear(16, self.out_dim)\n",
    "        self.lin1_2 = nn.Linear(2 * in_dim * in_dim, 16)\n",
    "        self.lin2_2 = nn.Linear(16, self.out_dim)\n",
    "        self.losses = []\n",
    "        self.losses1 = []\n",
    "        self.losses2 = []\n",
    "        self.mseLoss = nn.MSELoss()\n",
    "        self.reset_weigths()\n",
    "        self.nums = 3\n",
    "        # 1 666 3 663\n",
    "        with open(\"regions2.txt\", \"r\") as f:\n",
    "            counts = 0\n",
    "            tmp_list = []\n",
    "            for line in f:  # 116\n",
    "                if counts == 0:\n",
    "                    counts += 1\n",
    "                    continue\n",
    "                tmp = np.zeros(self.nums)\n",
    "                line.strip(\"\\n\")\n",
    "                line = line.split()\n",
    "\n",
    "                for columns in range(self.nums):\n",
    "                    # if columns != 2:\n",
    "                    #     break\n",
    "                    tmp[columns] = line[columns]\n",
    "\n",
    "                tmp_list.append(tmp)\n",
    "                counts += 1\n",
    "\n",
    "        self.R = np.array(tmp_list).transpose((1, 0))\n",
    "        self.R = torch.FloatTensor(self.R)\n",
    "        self.ij = []\n",
    "        print(self.R.shape)  # 6*116\n",
    "        for ri in range(self.nums):\n",
    "            tmp_sum = 0\n",
    "            temp = []\n",
    "            for i in range(dim):\n",
    "                for j in range(i + 1, dim):\n",
    "                    if self.R[ri][i] != 0 and self.R[ri][j] != 0:\n",
    "                        temp.append((i, j))\n",
    "            self.ij.append(temp)\n",
    "\n",
    "    def dim_reduce(\n",
    "        self,\n",
    "        adj_matrix,\n",
    "        num_reduce,\n",
    "        ortho_penalty,\n",
    "        variance_penalty,\n",
    "        neg_penalty,\n",
    "        kernel,\n",
    "        tell=None,\n",
    "    ):\n",
    "        kernel_p = torch.nn.functional.relu(kernel)\n",
    "        batch_size = int(adj_matrix.shape[0])\n",
    "        AF = torch.tensordot(adj_matrix, kernel_p, [[-1], [0]])\n",
    "        reduced_adj_matrix = torch.transpose(\n",
    "            torch.tensordot(kernel_p, AF, [[0], [1]]),  # num_reduce*batch*num_reduce\n",
    "            1,\n",
    "            0,\n",
    "        )  # num_reduce*batch*num_reduce*num_reduce\n",
    "        kernel_p_tran = kernel_p.transpose(-1, -2)  # num_reduce * column_dim\n",
    "        gram_matrix = torch.matmul(kernel_p_tran, kernel_p)\n",
    "        diag_elements = gram_matrix.diag()\n",
    "\n",
    "        if tell == \"A\":\n",
    "            if ortho_penalty != 0:\n",
    "                ortho_loss_matrix = torch.square(\n",
    "                    gram_matrix - torch.diag(diag_elements)\n",
    "                )\n",
    "                ortho_loss = torch.multiply(\n",
    "                    torch.tensor(ortho_penalty), torch.sum(ortho_loss_matrix)\n",
    "                )\n",
    "                self.losses.append(ortho_loss)\n",
    "\n",
    "            if variance_penalty != 0:\n",
    "                variance = diag_elements.var()\n",
    "                variance_loss = torch.multiply(torch.tensor(variance_penalty), variance)\n",
    "                self.losses.append(variance_loss)\n",
    "\n",
    "            if neg_penalty != 0:\n",
    "                neg_loss = torch.multiply(\n",
    "                    torch.tensor(neg_penalty),\n",
    "                    torch.sum(torch.nn.functional.relu(torch.tensor(1e-6) - kernel)),\n",
    "                )\n",
    "                self.losses.append(neg_loss)\n",
    "            self.losses.append(0.05 * torch.sum(torch.abs(kernel_p)))\n",
    "        elif tell == \"A1\":\n",
    "            if ortho_penalty != 0:\n",
    "                ortho_loss_matrix = torch.square(\n",
    "                    gram_matrix - torch.diag(diag_elements)\n",
    "                )\n",
    "                ortho_loss = torch.multiply(\n",
    "                    torch.tensor(ortho_penalty), torch.sum(ortho_loss_matrix)\n",
    "                )\n",
    "                self.losses1.append(ortho_loss)\n",
    "\n",
    "            if variance_penalty != 0:\n",
    "                variance = diag_elements.var()\n",
    "                variance_loss = torch.multiply(torch.tensor(variance_penalty), variance)\n",
    "                self.losses1.append(variance_loss)\n",
    "\n",
    "            if neg_penalty != 0:\n",
    "                neg_loss = torch.multiply(\n",
    "                    torch.tensor(neg_penalty),\n",
    "                    torch.sum(torch.nn.functional.relu(torch.tensor(1e-6) - kernel)),\n",
    "                )\n",
    "                self.losses1.append(neg_loss)\n",
    "            self.losses1.append(0.05 * torch.sum(torch.abs(kernel_p)))\n",
    "        elif tell == \"A2\":\n",
    "            if ortho_penalty != 0:\n",
    "                ortho_loss_matrix = torch.square(\n",
    "                    gram_matrix - torch.diag(diag_elements)\n",
    "                )\n",
    "                ortho_loss = torch.multiply(\n",
    "                    torch.tensor(ortho_penalty), torch.sum(ortho_loss_matrix)\n",
    "                )\n",
    "                self.losses2.append(ortho_loss)\n",
    "\n",
    "            if variance_penalty != 0:\n",
    "                variance = diag_elements.var()\n",
    "                variance_loss = torch.multiply(torch.tensor(variance_penalty), variance)\n",
    "                self.losses2.append(variance_loss)\n",
    "\n",
    "            if neg_penalty != 0:\n",
    "                neg_loss = torch.multiply(\n",
    "                    torch.tensor(neg_penalty),\n",
    "                    torch.sum(torch.nn.functional.relu(torch.tensor(1e-6) - kernel)),\n",
    "                )\n",
    "                self.losses2.append(neg_loss)\n",
    "            self.losses2.append(0.05 * torch.sum(torch.abs(kernel_p)))\n",
    "        return reduced_adj_matrix\n",
    "\n",
    "    def reset_weigths(self):\n",
    "        \"\"\"reset weights\"\"\"\n",
    "        stdv = 1.0 / math.sqrt(dim)\n",
    "        for weight in self.parameters():\n",
    "            init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "    def forward(self, A, A1, A2):\n",
    "        ##############################3\n",
    "        A = torch.transpose(A, 1, 0)\n",
    "        s_feature_p = A[0]\n",
    "        s_feature_n = A[1]\n",
    "        A1 = torch.transpose(A1, 1, 0)\n",
    "        s_feature_p1 = A1[0]\n",
    "        s_feature_n1 = A1[1]\n",
    "        A2 = torch.transpose(A2, 1, 0)\n",
    "        s_feature_p2 = A2[0]\n",
    "        s_feature_n2 = A2[1]\n",
    "        ###############################\n",
    "\n",
    "        ###############################\n",
    "        p_reduce = self.dim_reduce(\n",
    "            s_feature_p, 10, 0.2, 0.3, 0.1, self.kernel_p, tell=\"A\"\n",
    "        )\n",
    "        p_conv1_1_shared = self.gcn_p_shared(None, p_reduce)  # shared GCN\n",
    "        p_conv1 = self.gcn1_p(None, p_reduce)\n",
    "        p_conv1 = p_conv1 + p_conv1_1_shared  # sum\n",
    "        # p_conv1 = torch.cat((p_conv1, p_conv1_1_shared), -1) # concat\n",
    "        p_conv2_1_shared = self.gcn_p_shared1(p_conv1, p_reduce)\n",
    "        p_conv2 = self.gcn2_p(p_conv1, p_reduce)\n",
    "        # p_conv2_1_shared = self.gcn_p_shared1(p_conv2, p_reduce)\n",
    "        p_conv2 = p_conv2 + p_conv2_1_shared\n",
    "        # p_conv3 = self.gcn3_p(p_conv2, p_reduce)\n",
    "        n_reduce = self.dim_reduce(\n",
    "            s_feature_n, 10, 0.2, 0.5, 0.1, self.kernel_n, tell=\"A\"\n",
    "        )\n",
    "        n_conv1_1_shared = self.gcn_n_shared(None, n_reduce)  # shared GCN\n",
    "        n_conv1 = self.gcn1_n(None, n_reduce)\n",
    "        n_conv1 = n_conv1 + n_conv1_1_shared  # sum\n",
    "        # n_conv1 = torch.cat((n_conv1, n_conv1_1_shared), -1) # concat\n",
    "        n_conv2 = self.gcn2_n(n_conv1, n_reduce)\n",
    "        n_conv2_1_shared = self.gcn_n_shared1(n_conv1, n_reduce)\n",
    "        # n_conv2_1_shared = self.gcn_n_shared1(n_conv1, n_reduce)\n",
    "        n_conv2 = n_conv2 + n_conv2_1_shared\n",
    "        # n_conv3 = self.gcn3_n(n_conv2, n_reduce)\n",
    "        # ---------------------------------\n",
    "        p_reduce1 = self.dim_reduce(\n",
    "            s_feature_p1, 10, 0.2, 0.3, 0.1, self.kernel_p1, tell=\"A1\"\n",
    "        )\n",
    "        p_conv1_2_shared = self.gcn_p_shared(None, p_reduce1)  # shared GCN\n",
    "        p_conv1_1 = self.gcn1_p_1(None, p_reduce1)\n",
    "        p_conv1_1 = p_conv1_1 + p_conv1_2_shared  # sum\n",
    "        # p_conv1_1 = torch.cat((p_conv1_1, p_conv1_2_shared), -1) # concat\n",
    "        p_conv2_1 = self.gcn2_p_1(p_conv1_1, p_reduce1)\n",
    "        p_conv2_2_shared = self.gcn_p_shared1(p_conv1_1, p_reduce1)\n",
    "        # p_conv2_2_shared = self.gcn_p_shared1(p_conv1_1, p_reduce1)\n",
    "        p_conv2_1 = p_conv2_1 + p_conv2_2_shared\n",
    "        # p_conv3 = self.gcn3_p(p_conv2, p_reduce)\n",
    "        n_reduce1 = self.dim_reduce(\n",
    "            s_feature_n1, 10, 0.2, 0.5, 0.1, self.kernel_n1, tell=\"A1\"\n",
    "        )\n",
    "        n_conv1_2_shared = self.gcn_n_shared(None, n_reduce1)  # shared GCN\n",
    "        n_conv1_1 = self.gcn1_n_1(None, n_reduce1)\n",
    "        n_conv1_1 = n_conv1_1 + n_conv1_2_shared  # sum\n",
    "        # n_conv1_1 = torch.cat((n_conv1_1, n_conv1_2_shared), -1) #concat\n",
    "        n_conv2_1 = self.gcn2_n_1(n_conv1_1, n_reduce1)\n",
    "        n_conv2_2_shared = self.gcn_n_shared1(n_conv1_1, n_reduce1)\n",
    "        # n_conv2_2_shared = self.gcn_n_shared1(n_conv1_1, n_reduce1)\n",
    "        n_conv2_1 = n_conv2_1 + n_conv2_2_shared\n",
    "        # n_conv3 = self.gcn3_n(n_conv2, n_reduce)\n",
    "        # ---------------------------------\n",
    "        p_reduce2 = self.dim_reduce(\n",
    "            s_feature_p2, 10, 0.2, 0.3, 0.1, self.kernel_p2, tell=\"A2\"\n",
    "        )\n",
    "        p_conv1_3_shared = self.gcn_p_shared(None, p_reduce2)  # shared GCN\n",
    "        p_conv1_2 = self.gcn1_p_2(None, p_reduce2)\n",
    "        p_conv1_2 = p_conv1_2 + p_conv1_3_shared  # sum\n",
    "        # p_conv1_2 = torch.cat((p_conv1_2, p_conv1_3_shared), -1) # concat\n",
    "        p_conv2_2 = self.gcn2_p_2(p_conv1_2, p_reduce2)\n",
    "        p_conv2_3_shared = self.gcn_p_shared1(p_conv1_2, p_reduce2)\n",
    "        # p_conv2_3_shared = self.gcn_p_shared1(p_conv1_2, p_reduce2)\n",
    "        p_conv2_2 = p_conv2_2 + p_conv2_3_shared\n",
    "        # p_conv3 = self.gcn3_p(p_conv2, p_reduce)\n",
    "        n_reduce2 = self.dim_reduce(\n",
    "            s_feature_n2, 10, 0.2, 0.5, 0.1, self.kernel_n2, tell=\"A2\"\n",
    "        )\n",
    "        n_conv1_3_shared = self.gcn_n_shared(None, n_reduce2)\n",
    "        n_conv1_2 = self.gcn1_n_2(None, n_reduce2)\n",
    "        n_conv1_2 = n_conv1_2 + n_conv1_3_shared  # sum\n",
    "        # n_conv1_2 = torch.cat((n_conv1_2, n_conv1_3_shared), -1) # concat\n",
    "        n_conv2_2 = self.gcn2_n_2(n_conv1_2, n_reduce2)\n",
    "        n_conv2_3_shared = self.gcn_n_shared1(n_conv1_2, n_reduce2)\n",
    "        # n_conv2_3_shared = self.gcn_n_shared1(n_conv1_2, n_reduce2)\n",
    "        n_conv2_2 = n_conv2_2 + n_conv2_3_shared\n",
    "        # n_conv3 = self.gcn3_n(n_conv2, n_reduce)\n",
    "        # ----------------------------------\n",
    "        # p_conv1_1_shared = self.gcn_p_shared(None, p_reduce)\n",
    "        # p_conv1_2_shared = self.gcn_p_shared(None, p_reduce1)\n",
    "        # p_conv1_3_shared = self.gcn_p_shared(None, p_reduce2)\n",
    "        #\n",
    "        # n_conv1_1_shared = self.gcn_n_shared(None, n_reduce)\n",
    "        # n_conv1_2_shared = self.gcn_n_shared(None, n_reduce1)\n",
    "        # n_conv1_3_shared = self.gcn_n_shared(None, n_reduce2)\n",
    "        # -----------------------------------\n",
    "        # p_conv = p_conv2 + p_conv2_1 + p_conv2_2\n",
    "        # n_conv = n_conv2 + n_conv2_1 + n_conv2_2\n",
    "        ##################################\n",
    "\n",
    "        # conv_concat = torch.cat([p_conv2, n_conv2], -1).reshape([-1, 2 * self.in_dim * self.in_dim])\n",
    "        conv_concat = torch.cat([p_conv2, n_conv2], -1).reshape(\n",
    "            [-1, 2 * self.in_dim * self.in_dim]\n",
    "        )\n",
    "        conv_concat1 = torch.cat([p_conv2_1, n_conv2_1], -1).reshape(\n",
    "            [-1, 2 * self.in_dim * self.in_dim]\n",
    "        )\n",
    "        conv_concat2 = torch.cat([p_conv2_2, n_conv2_2], -1).reshape(\n",
    "            [-1, 2 * self.in_dim * self.in_dim]\n",
    "        )\n",
    "        output = self.lin2(self.lin1(conv_concat))\n",
    "        output1 = self.lin2_1(self.lin1_1(conv_concat1))\n",
    "        output2 = self.lin2_2(self.lin1_2(conv_concat2))\n",
    "        # output = torch.softmax(output, dim=1)\n",
    "\n",
    "        # F loss\n",
    "        simi_loss1 = self.SimiLoss(self.kernel_p, self.kernel_p1)\n",
    "        simi_loss2 = self.SimiLoss(self.kernel_p, self.kernel_p2)\n",
    "        simi_loss3 = self.SimiLoss(self.kernel_p1, self.kernel_p2)\n",
    "        simi_loss4 = self.SimiLoss(self.kernel_n, self.kernel_n1)\n",
    "        simi_loss5 = self.SimiLoss(self.kernel_n, self.kernel_n2)\n",
    "        simi_loss6 = self.SimiLoss(self.kernel_n1, self.kernel_n2)\n",
    "        # simi_loss = squ_p1.sum() + squ_p2.sum() + squ_p3.sum() + squ_n1.sum() + squ_n2.sum() + squ_n3.sum()\n",
    "        simiLoss = 0.0 * (\n",
    "            simi_loss6 + simi_loss4 + simi_loss3 + simi_loss2 + simi_loss1 + simi_loss5\n",
    "        )\n",
    "        # 0.2 674 0.1 683 0.3 669 0.4 668 0.5 660 0.05 674 0.08 673 0.15 673\n",
    "\n",
    "        score, score1, score2, score_, score_1, score_2, l1, l2, l3, l4, l5, l6 = (\n",
    "            self.load_s_c(\n",
    "                self.kernel_p,\n",
    "                self.kernel_p1,\n",
    "                self.kernel_p2,\n",
    "                self.kernel_n,\n",
    "                self.kernel_n1,\n",
    "                self.kernel_n2,\n",
    "            )\n",
    "        )\n",
    "        # score, score1, score2, score_, score_1, score_2 = self.SimiLoss3(self.kernel_p, self.kernel_p1, self.kernel_p2,\n",
    "        #                                                                  self.kernel_n, self.kernel_n1, self.kernel_n2)\n",
    "        # 约束score max\n",
    "        loss1 = -1 * torch.log(score + 1e-3)\n",
    "        loss2 = -1 * torch.log(score1 + 1e-3)\n",
    "        loss3 = -1 * torch.log(score2 + 1e-3)\n",
    "        loss4 = -1 * torch.log(score_ + 1e-3)\n",
    "        loss5 = -1 * torch.log(score_1 + 1e-3)\n",
    "        loss6 = -1 * torch.log(score_2 + 1e-3)\n",
    "        l = loss1 + loss2 + loss3 + loss4 + loss5 + loss6\n",
    "        ll = l1 + l2 + l3 + l4 + l5 + l6\n",
    "        # 625\n",
    "\n",
    "        # score = self.load_s_c(self.kernel_p)\n",
    "        # score1 = self.load_s_c(self.kernel_p1)\n",
    "        # score2 = self.load_s_c(self.kernel_p2)\n",
    "        SimiLoss1 = self.SimiLoss2(score, score1)\n",
    "        SimiLoss2 = self.SimiLoss2(score, score2)\n",
    "        SimiLoss3 = self.SimiLoss2(score1, score2)\n",
    "        # score = score.view(6,1)\n",
    "        # SimiLoss1 = F.cross_entropy(score.view(6, 1), score1.view(6, 1))\n",
    "\n",
    "        # score_ = self.load_s_c(self.kernel_n)\n",
    "        # score_1 = self.load_s_c(self.kernel_n1)\n",
    "        # score_2 = self.load_s_c(self.kernel_n2)\n",
    "        SimiLoss4 = self.SimiLoss2(score_, score_1)\n",
    "        SimiLoss5 = self.SimiLoss2(score_, score_2)\n",
    "        SimiLoss6 = self.SimiLoss2(score_1, score_2)\n",
    "        simiLoss2 = 0 * (\n",
    "            SimiLoss1 + SimiLoss6 + SimiLoss5 + SimiLoss4 + SimiLoss2 + SimiLoss3\n",
    "        )\n",
    "        simiLoss = simiLoss + l.sum() * 0.00 + ll.sum() * 0.1\n",
    "        # simiLoss F相似性约束 0.1     l 子网络max    ll 子网络离散化\n",
    "\n",
    "        # score max 0.01 683 0.001 677\n",
    "        # 0.01l 0.1ll 663    0.1simiLoss 0.01l 0.1ll 662        0.01 0.01l 666           0.001 0.01 660     0.01 0.1 669\n",
    "        # 0.01 1 656     0.01 0.5 655\n",
    "        loss = torch.sum(torch.tensor(self.losses)) + simiLoss\n",
    "        self.losses.clear()\n",
    "        loss1 = torch.sum(torch.tensor(self.losses1))\n",
    "        self.losses1.clear()\n",
    "        loss2 = torch.sum(torch.tensor(self.losses2))\n",
    "        self.losses2.clear()\n",
    "        return output, output1, output2, loss, loss1, loss2\n",
    "\n",
    "    def SimiLoss(self, F1, F2):\n",
    "        f1 = torch.nn.functional.relu(F1)\n",
    "        f2 = torch.nn.functional.relu(F2).T\n",
    "        O = torch.matmul(f1, f2)\n",
    "        # O = O.trace()\n",
    "        O0 = O.diagonal()\n",
    "        O1 = F.softmax(O0)\n",
    "        O2 = torch.log(O1).sum()\n",
    "        # U = F.relu(O)\n",
    "        # U1 = U.trace()\n",
    "        # T = U.sum()\n",
    "        # simi_loss1 = -torch.log(U1)\n",
    "        simi_loss1 = -O2\n",
    "\n",
    "        ####\n",
    "        # simi_loss1 = 0\n",
    "        # f1 = torch.nn.functional.relu(F1).T\n",
    "        # f2 = torch.nn.functional.relu(F2).T\n",
    "        # for i in range(8):\n",
    "        #     L = nn.CrossEntropyLoss()\n",
    "        #     l = L(f1[i], f2[i])\n",
    "        #     simi_loss1 += l\n",
    "        ####\n",
    "        return simi_loss1\n",
    "\n",
    "    def SimiLoss2(self, S1, S2):\n",
    "        # CE loss\n",
    "        # s1 = S1.unsqueeze(0)\n",
    "        # s2 = S2.unsqueeze(0).T\n",
    "        # O = torch.matmul(s1, s2)\n",
    "        # # O = O.trace()\n",
    "        # O1 = F.softmax(O)\n",
    "        # O2 = torch.log(O1)\n",
    "        # # U = F.relu(O)\n",
    "        # # U1 = U.trace()\n",
    "        # # T = U.sum()\n",
    "        # # simi_loss1 = -torch.log(U1)\n",
    "        # simi_loss2 = -O2\n",
    "\n",
    "        # MSE loss\n",
    "        # s1 = S1.unsqueeze(0)\n",
    "        # s2 = S2.unsqueeze(0)\n",
    "        # simi_loss2 = (abs(s1 - s2)).sum()\n",
    "\n",
    "        s1 = S1.unsqueeze(1)\n",
    "        s2 = S2.unsqueeze(1)\n",
    "        # s1 = torch.log(S1.unsqueeze(1) + 0.0001)\n",
    "        # s2 = torch.log(S2.unsqueeze(1) + 0.0001)\n",
    "        simi_loss2 = self.mseLoss(s1, s2)\n",
    "        return simi_loss2\n",
    "\n",
    "    def load_s_c(self, F, F1, F2, F3, F4, F5):\n",
    "        F = torch.nn.functional.relu(F).T\n",
    "        F1 = torch.nn.functional.relu(F1).T\n",
    "        F2 = torch.nn.functional.relu(F2).T\n",
    "        F3 = torch.nn.functional.relu(F3).T\n",
    "        F4 = torch.nn.functional.relu(F4).T\n",
    "        F5 = torch.nn.functional.relu(F5).T\n",
    "        s = F  # 5 * 116\n",
    "        s1 = F1  # 5 * 116\n",
    "        s2 = F2  # 5 * 116\n",
    "        s3 = F3  # 5 * 116\n",
    "        s4 = F4  # 5 * 116\n",
    "        s5 = F5  # 5 * 116\n",
    "        s_MAX_INDEX = torch.argmax(s, dim=0)\n",
    "        s_MAX_INDEX1 = torch.argmax(s1, dim=0)\n",
    "        s_MAX_INDEX2 = torch.argmax(s2, dim=0)\n",
    "        s_MAX_INDEX3 = torch.argmax(s3, dim=0)\n",
    "        s_MAX_INDEX4 = torch.argmax(s4, dim=0)\n",
    "        s_MAX_INDEX5 = torch.argmax(s5, dim=0)\n",
    "        # print(s_MAX_INDEX)\n",
    "        # ss = np.zeros((5, 116))\n",
    "        ss = torch.zeros((self.in_dim, dim)).to(device)\n",
    "        ss1 = torch.zeros((self.in_dim, dim)).to(device)\n",
    "        ss2 = torch.zeros((self.in_dim, dim)).to(device)\n",
    "        ss3 = torch.zeros((self.in_dim, dim)).to(device)\n",
    "        ss4 = torch.zeros((self.in_dim, dim)).to(device)\n",
    "        ss5 = torch.zeros((self.in_dim, dim)).to(device)\n",
    "        for ii in range(dim):\n",
    "            ss[s_MAX_INDEX[ii]][ii] = s[s_MAX_INDEX[ii]][ii]\n",
    "            ss1[s_MAX_INDEX1[ii]][ii] = s1[s_MAX_INDEX1[ii]][ii]\n",
    "            ss2[s_MAX_INDEX2[ii]][ii] = s2[s_MAX_INDEX2[ii]][ii]\n",
    "            ss3[s_MAX_INDEX3[ii]][ii] = s3[s_MAX_INDEX3[ii]][ii]\n",
    "            ss4[s_MAX_INDEX4[ii]][ii] = s4[s_MAX_INDEX4[ii]][ii]\n",
    "            ss5[s_MAX_INDEX5[ii]][ii] = s5[s_MAX_INDEX5[ii]][ii]\n",
    "        # s = ss\n",
    "        # s1 = ss1\n",
    "        # s2 = ss2\n",
    "        # s3 = ss3\n",
    "        # s4 = ss4\n",
    "        # s5 = ss5\n",
    "        R_sum = torch.sum(self.R, dim=1)\n",
    "        scores = torch.zeros(self.nums).to(device)\n",
    "        scores_ = torch.zeros(self.nums).to(device)\n",
    "        scores1 = torch.zeros(self.nums).to(device)\n",
    "        scores1_ = torch.zeros(self.nums).to(device)\n",
    "        scores2 = torch.zeros(self.nums).to(device)\n",
    "        scores2_ = torch.zeros(self.nums).to(device)\n",
    "        scores3 = torch.zeros(self.nums).to(device)\n",
    "        scores3_ = torch.zeros(self.nums).to(device)\n",
    "        scores4 = torch.zeros(self.nums).to(device)\n",
    "        scores4_ = torch.zeros(self.nums).to(device)\n",
    "        scores5 = torch.zeros(self.nums).to(device)\n",
    "        scores5_ = torch.zeros(self.nums).to(device)\n",
    "        for ri in range(self.nums):\n",
    "            tmp_sum = 0\n",
    "            tmp_sum1 = 0\n",
    "            tmp_sum2 = 0\n",
    "            tmp_sum3 = 0\n",
    "            tmp_sum4 = 0\n",
    "            tmp_sum5 = 0\n",
    "\n",
    "            tmp_sum_ = 0\n",
    "            tmp_sum_1 = 0\n",
    "            tmp_sum_2 = 0\n",
    "            tmp_sum_3 = 0\n",
    "            tmp_sum_4 = 0\n",
    "            tmp_sum_5 = 0\n",
    "            temp = self.ij[ri]\n",
    "            for ij in temp:\n",
    "                i = ij[0]\n",
    "                j = ij[1]\n",
    "\n",
    "                t = s[:, i] * s[:, j]\n",
    "                tmp_sum_ += t.sum()\n",
    "                t1 = s1[:, i] * s1[:, j]\n",
    "                tmp_sum_1 += t1.sum()\n",
    "                t2 = s2[:, i] * s2[:, j]\n",
    "                tmp_sum_2 += t2.sum()\n",
    "                t3 = s3[:, i] * s3[:, j]\n",
    "                tmp_sum_3 += t3.sum()\n",
    "                t4 = s4[:, i] * s4[:, j]\n",
    "                tmp_sum_4 += t4.sum()\n",
    "                t5 = s5[:, i] * s5[:, j]\n",
    "                tmp_sum_5 += t5.sum()\n",
    "\n",
    "                t = torch.matmul(ss[:, i].unsqueeze(1), ss[:, j].unsqueeze(1).T)\n",
    "                t = t - torch.diag_embed(torch.diag(t))\n",
    "                tmp_sum += t.sum()\n",
    "                t1 = torch.matmul(ss1[:, i].unsqueeze(1), ss1[:, j].unsqueeze(1).T)\n",
    "                t1 = t1 - torch.diag_embed(torch.diag(t1))\n",
    "                tmp_sum1 += t1.sum()\n",
    "                t2 = torch.matmul(ss2[:, i].unsqueeze(1), ss2[:, j].unsqueeze(1).T)\n",
    "                t2 = t2 - torch.diag_embed(torch.diag(t2))\n",
    "                tmp_sum2 += t2.sum()\n",
    "                t3 = torch.matmul(ss3[:, i].unsqueeze(1), ss3[:, j].unsqueeze(1).T)\n",
    "                t3 = t3 - torch.diag_embed(torch.diag(t3))\n",
    "                tmp_sum3 += t3.sum()\n",
    "                t4 = torch.matmul(ss4[:, i].unsqueeze(1), ss4[:, j].unsqueeze(1).T)\n",
    "                t4 = t4 - torch.diag_embed(torch.diag(t4))\n",
    "                tmp_sum4 += t4.sum()\n",
    "                t5 = torch.matmul(ss5[:, i].unsqueeze(1), ss5[:, j].unsqueeze(1).T)\n",
    "                t5 = t5 - torch.diag_embed(torch.diag(t5))\n",
    "                tmp_sum5 += t5.sum()\n",
    "\n",
    "            scores_[ri] = tmp_sum_\n",
    "            scores1_[ri] = tmp_sum_1\n",
    "            scores2_[ri] = tmp_sum_2\n",
    "            scores3_[ri] = tmp_sum_3\n",
    "            scores4_[ri] = tmp_sum_4\n",
    "            scores5_[ri] = tmp_sum_5\n",
    "\n",
    "            scores[ri] = (2 / (R_sum[ri] ** 2)) * tmp_sum\n",
    "            scores1[ri] = (2 / (R_sum[ri] ** 2)) * tmp_sum1\n",
    "            scores2[ri] = (2 / (R_sum[ri] ** 2)) * tmp_sum2\n",
    "            scores3[ri] = (2 / (R_sum[ri] ** 2)) * tmp_sum3\n",
    "            scores4[ri] = (2 / (R_sum[ri] ** 2)) * tmp_sum4\n",
    "            scores5[ri] = (2 / (R_sum[ri] ** 2)) * tmp_sum5\n",
    "        return (\n",
    "            scores,\n",
    "            scores1,\n",
    "            scores2,\n",
    "            scores3,\n",
    "            scores4,\n",
    "            scores5,\n",
    "            scores_,\n",
    "            scores1_,\n",
    "            scores2_,\n",
    "            scores3_,\n",
    "            scores4_,\n",
    "            scores5_,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8c4882c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 200])\n"
     ]
    }
   ],
   "source": [
    "model = model_gnn(200, 200, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffefa791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([1, 4])\n",
      "Output1: tensor([[-31.6323, -21.4935, -44.9187, -18.1236]])\n",
      "Output2: tensor([[-75.5426,   4.3901,  -8.4391,  54.8549]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bv/7h0f0hns2y72yqgg3ww8qkkm0000gn/T/ipykernel_6743/258970988.py:382: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  O1 = F.softmax(O0)\n"
     ]
    }
   ],
   "source": [
    "adj_size = 200  # Adjust based on your model's expected input size\n",
    "\n",
    "# Create a zero-filled adjacency matrix\n",
    "zero_adj = torch.zeros((1, adj_size, adj_size))  # Shape: (batch_size, num_nodes, num_nodes)\n",
    "\n",
    "# Ensure it's on the correct device (CPU or GPU)\n",
    "zero_adj = zero_adj.to(device)\n",
    "\n",
    "# If the model expects multiple adjacency matrices, you can create the other ones as zeros as well\n",
    "# For example, let's assume the model takes `adj`, `adj1`, `adj2`\n",
    "zero_adj1 = zero_adj\n",
    "zero_adj2 = zero_adj\n",
    "\n",
    "# Pass the zero-filled adjacency matrices through the model to get the output\n",
    "model.eval()  # Set the model to evaluation mode (important to disable dropout, etc.)\n",
    "with torch.no_grad():  # Disable gradient tracking\n",
    "    output, output1, output2, loss, loss1, loss2 = model(zero_adj, zero_adj1, zero_adj2)\n",
    "\n",
    "# Print the outputs\n",
    "print(\"Output:\", output.shape)\n",
    "print(\"Output1:\", output1)\n",
    "print(\"Output2:\", output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b85aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
