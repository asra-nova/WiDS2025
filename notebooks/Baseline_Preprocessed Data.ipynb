{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import umap.umap_ as umap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from gc import collect\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import RFECV, SequentialFeatureSelector, SelectKBest\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import f1_score, roc_auc_score, roc_curve, make_scorer, r2_score, accuracy_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer, PolynomialFeatures, MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer, MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = \"../widsdatathon2025/Preprocessed/preprocessed_selected_features\"\n",
    "aux_file_name = \"aux.csv\"\n",
    "connectome_matrices_file_name = \"connectome_matrices.csv\"\n",
    "\n",
    "def get_feats(mode=\"train\"):\n",
    "\n",
    "    feats = pd.read_csv(f\"{preprocessed_data}/{mode}/{aux_file_name}\")\n",
    "    # conns = pd.read_csv(f\"{preprocessed_data}/{mode}/{connectome_matrices_file_name}\")\n",
    "    # feats = feats.merge(conns, on=\"participant_id\", how=\"left\")\n",
    "\n",
    "    if mode == \"train\":\n",
    "        labels = pd.read_csv(f\"{preprocessed_data}/{mode}/labels.csv\")\n",
    "        feats = feats.merge(labels, on=\"participant_id\", how=\"left\")\n",
    "        return feats, labels\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "train,y = get_feats(mode=\"train\")\n",
    "test = get_feats(mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.set_index('participant_id',inplace=True)\n",
    "test.set_index('participant_id',inplace=True)\n",
    "targets = ['ADHD_Outcome','Sex_F']\n",
    "features = test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_nulls(df):\n",
    "  if df.isnull().any().any():\n",
    "    print(\"The DataFrame contains null values.\")\n",
    "  else:\n",
    "    print(\"The DataFrame does not contain null values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame does not contain null values.\n",
      "The DataFrame does not contain null values.\n",
      "Train: (1213, 65), Test: (1213, 63)\n"
     ]
    }
   ],
   "source": [
    "check_for_nulls(train)\n",
    "check_for_nulls(test)\n",
    "print(f'Train: {train.shape}, Test: {test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train.drop(targets,axis=1), \n",
    "                                                    y[targets], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "log_features = [f for f in features if (train[f] >= 0).all() and scipy.stats.skew(train[f]) > 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ridge Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ridge Classifier Results:\n",
      "Accuracy: 0.6008230452674898\n",
      "Recall (Sensitivity): 0.48447253433208487\n",
      "Specificity: 0.7278452541610436\n",
      "F1 Score: 0.7483588621444202\n"
     ]
    }
   ],
   "source": [
    "model = MultiOutputClassifier(make_pipeline(RidgeClassifier(alpha=100)))\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "sensitivity = recall_score(y_test, y_pred, average=None)  \n",
    "\n",
    "specificities = []\n",
    "for i in range(y_test.shape[1]):\n",
    "    cm = confusion_matrix(y_test.iloc[:, i], y_pred[:, i])  \n",
    "    if cm.shape == (2, 2):  \n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0  \n",
    "    else:\n",
    "        specificity = 0 \n",
    "    specificities.append(specificity)\n",
    "\n",
    "# Print Results\n",
    "print('\\nRidge Classifier Results:')\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Recall (Sensitivity):', sensitivity.mean())  # Mean sensitivity across labels\n",
    "print('Specificity:', np.mean(specificities))  # Mean specificity across labels\n",
    "print('F1 Score:', f1_score(y_test, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Results:\n",
      "Accuracy: 0.5843621399176955\n",
      "Recall (Sensitivity): 0.6271847690387016\n",
      "Specificity: 0.7199730094466936\n",
      "F1 Score: 0.7551020408163265\n"
     ]
    }
   ],
   "source": [
    "model = MultiOutputClassifier(make_pipeline(LogisticRegression(max_iter=10000)))\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "sensitivity = recall_score(y_test, y_pred, average=None)  \n",
    "\n",
    "specificities = []\n",
    "for i in range(y_test.shape[1]):  \n",
    "    cm = confusion_matrix(y_test.iloc[:, i], y_pred[:, i])  \n",
    "    if cm.shape == (2, 2): \n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0  \n",
    "    else:\n",
    "        specificity = 0 \n",
    "    specificities.append(specificity)\n",
    "\n",
    "# Print Results\n",
    "print('\\nLogistic Regression Results:')\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Recall (Sensitivity):', sensitivity.mean()) \n",
    "print('Specificity:', np.mean(specificities)) \n",
    "print('F1 Score:', f1_score(y_test, y_pred, average='micro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Results:\n",
      "Accuracy: 0.5761316872427984\n",
      "Recall (Sensitivity): 0.5021847690387016\n",
      "Specificity: 0.7506072874493928\n",
      "F1 Score: 0.734065934065934\n"
     ]
    }
   ],
   "source": [
    "rf_model = MultiOutputClassifier(make_pipeline(RandomForestClassifier(n_estimators=100, random_state=42))) \n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "\n",
    "sensitivity = recall_score(y_test, y_pred, average=None)  \n",
    "\n",
    "specificities = []\n",
    "for i in range(y_test.shape[1]):  \n",
    "    cm = confusion_matrix(y_test.iloc[:, i], y_pred[:, i])  \n",
    "    if cm.shape == (2, 2):  \n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0 \n",
    "    else:\n",
    "        specificity = 0  \n",
    "    specificities.append(specificity)\n",
    "\n",
    "# Print Results\n",
    "print('\\nRandom Forest Results:')\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Recall (Sensitivity):', sensitivity.mean())  \n",
    "print('Specificity:', np.mean(specificities))  \n",
    "print('F1 Score:', f1_score(y_test, y_pred, average='micro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Kernel SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel SVM Results:\n",
      "Accuracy: 0.6008230452674898\n",
      "Recall (Sensitivity): 0.47042759051186017\n",
      "Specificity: 0.7970760233918128\n",
      "F1 Score: 0.7494356659142212\n"
     ]
    }
   ],
   "source": [
    "svm_model = MultiOutputClassifier(make_pipeline(SVC(kernel='rbf', probability=True, random_state=42)  ))\n",
    "\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "sensitivity = recall_score(y_test, y_pred, average=None) \n",
    "\n",
    "specificities = []\n",
    "for i in range(y_test.shape[1]):  \n",
    "    cm = confusion_matrix(y_test.iloc[:, i], y_pred[:, i])  \n",
    "    if cm.shape == (2, 2):  \n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0 \n",
    "    else:\n",
    "        specificity = 0  \n",
    "    specificities.append(specificity)\n",
    "\n",
    "print('\\nKernel SVM Results:')\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Recall (Sensitivity):', sensitivity.mean()) \n",
    "print('Specificity:', np.mean(specificities))  \n",
    "print('F1 Score:', f1_score(y_test, y_pred, average='micro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Results:\n",
      "Accuracy: 0.5102880658436214\n",
      "Recall (Sensitivity): 0.6090043695380774\n",
      "Specificity: 0.6591992802519118\n",
      "F1 Score: 0.7258064516129032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amirsabbaghziarani/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [18:52:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/amirsabbaghziarani/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [18:52:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "xgb_model = MultiOutputClassifier(make_pipeline(\n",
    "\n",
    "    XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "))\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "sensitivity = recall_score(y_test, y_pred, average=None)\n",
    "\n",
    "specificities = []\n",
    "for i in range(y_test.shape[1]):  \n",
    "    cm = confusion_matrix(y_test.iloc[:, i], y_pred[:, i])  \n",
    "    if cm.shape == (2, 2):  \n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0 \n",
    "    else:\n",
    "        specificity = 0  \n",
    "    specificities.append(specificity)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print('\\nXGBoost Results:')\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Recall (Sensitivity):', sensitivity.mean()) \n",
    "print('Specificity:', np.mean(specificities)) \n",
    "print('F1 Score:', f1_score(y_test, y_pred, average='micro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Accuracy: 74.69%\n",
      "Recall: 74.69%\n",
      "F1 Score: 69.39%\n",
      "Fold 2\n",
      "Accuracy: 67.90%\n",
      "Recall: 67.90%\n",
      "F1 Score: 60.66%\n",
      "Fold 3\n",
      "Accuracy: 73.87%\n",
      "Recall: 73.87%\n",
      "F1 Score: 72.50%\n",
      "Fold 4\n",
      "Accuracy: 70.25%\n",
      "Recall: 70.25%\n",
      "F1 Score: 63.09%\n",
      "Fold 5\n",
      "Accuracy: 72.73%\n",
      "Recall: 72.73%\n",
      "F1 Score: 66.83%\n",
      "\n",
      "Cross-validation results:\n",
      "Mean Accuracy: 71.89%\n",
      "Mean Recall: 71.89%\n",
      "Mean F1 Score: 66.49%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "preprocessed_data = \"../widsdatathon2025/Preprocessed/preprocessed_selected_features\"\n",
    "aux_file_name = \"aux.csv\"\n",
    "labels_file_name = \"labels.csv\"\n",
    "\n",
    "# Load data\n",
    "def get_feats(mode=\"train\"):\n",
    "    feats = pd.read_csv(f\"{preprocessed_data}/{mode}/{aux_file_name}\")\n",
    "    if mode == \"train\":\n",
    "        labels = pd.read_csv(f\"{preprocessed_data}/{mode}/{labels_file_name}\")\n",
    "        feats = feats.merge(labels, on=\"participant_id\", how=\"left\")\n",
    "        return feats\n",
    "    return feats\n",
    "\n",
    "train = get_feats(mode=\"train\")\n",
    "\n",
    "# Define targets explicitly\n",
    "targets = ['ADHD_Outcome', 'Sex_F']\n",
    "\n",
    "# Prepare features and labels\n",
    "X = train.drop(['participant_id'] + targets, axis=1).values\n",
    "y = train[targets].values\n",
    "\n",
    "# Define neural network\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, layer_dims, output_dim1, output_dim2):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for dim in layer_dims:\n",
    "            layers.append(nn.Linear(prev_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = dim\n",
    "        self.shared_layers = nn.Sequential(*layers)\n",
    "        self.output1 = nn.Linear(prev_dim, output_dim1)\n",
    "        self.output2 = nn.Linear(prev_dim, output_dim2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shared_output = self.shared_layers(x)\n",
    "        return self.output1(shared_output), self.output2(shared_output)\n",
    "\n",
    "# Parameters\n",
    "layer_dims = [128 ,64 , 32]\n",
    "input_dim = X.shape[1]\n",
    "output_dims = [len(np.unique(y[:, 0])), len(np.unique(y[:, 1]))]\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold_metrics = {'accuracy': [], 'recall': [], 'f1': []}\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    print(f\"Fold {fold+1}\")\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Data normalization\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor_1 = torch.tensor(y_train[:, 0], dtype=torch.long)\n",
    "    y_train_tensor_2 = torch.tensor(y_train[:, 1], dtype=torch.long)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor_1 = torch.tensor(y_test[:, 0], dtype=torch.long)\n",
    "    y_test_tensor_2 = torch.tensor(y_test[:, 1], dtype=torch.long)\n",
    "\n",
    "    # Initialize model, criterion, and optimizer\n",
    "    model = NeuralNetwork(input_dim, layer_dims, output_dims[0], output_dims[1])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    epochs = 50\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs1, outputs2 = model(X_train_tensor)\n",
    "        loss = criterion(outputs1, y_train_tensor_1) + criterion(outputs2, y_train_tensor_2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred1, pred2 = model(X_test_tensor)\n",
    "        pred_classes1 = torch.argmax(pred1, axis=1)\n",
    "        pred_classes2 = torch.argmax(pred2, axis=1)\n",
    "\n",
    "    accuracy = (accuracy_score(y_test_tensor_1, pred_classes1) + accuracy_score(y_test_tensor_2, pred_classes2)) / 2\n",
    "    recall = (recall_score(y_test_tensor_1, pred_classes1, average='weighted') + recall_score(y_test_tensor_2, pred_classes2, average='weighted')) / 2\n",
    "    f1 = (f1_score(y_test_tensor_1, pred_classes1, average='weighted') + f1_score(y_test_tensor_2, pred_classes2, average='weighted')) / 2\n",
    "\n",
    "    fold_metrics['accuracy'].append(accuracy)\n",
    "    fold_metrics['recall'].append(recall)\n",
    "    fold_metrics['f1'].append(f1)\n",
    "\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "    print(f'Recall: {recall * 100:.2f}%')\n",
    "    print(f'F1 Score: {f1 * 100:.2f}%')\n",
    "\n",
    "# Cross-validation results\n",
    "print(\"\\nCross-validation results:\")\n",
    "print(f'Mean Accuracy: {np.mean(fold_metrics[\"accuracy\"]) * 100:.2f}%')\n",
    "print(f'Mean Recall: {np.mean(fold_metrics[\"recall\"]) * 100:.2f}%')\n",
    "print(f'Mean F1 Score: {np.mean(fold_metrics[\"f1\"]) * 100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
